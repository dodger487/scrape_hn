Last time, we went all the way from the Morse Code to the creation of Unicode. And Iâ€™ve left you there with a question. How can you fit millions of characters into an 8-bit encoding? Wasnâ€™t it exactly how that wild ride beganâ€Šâ€”â€Šwith short code encodings?

As promised, weâ€™ll now dive into the more technical part of the Unicode. Iâ€™ll try to keep it relatively simple, but itâ€™s going to be more complex than the previous part.

It isnâ€™t completely right (nor is it wrong) to say UTF-8 is an 8 bit encoding because itâ€™s a variable-length code. As the name suggests there wonâ€™t be a single length the code uses. In the case of UTF-8, you get code length varying from 8 do 32 bits.

But wasnâ€™t this the problem with the Morse Code? If one letter uses longer code than the other, how can you know which one is which? Itâ€™s actually not that hard.

First, we have to remember that weâ€™re operating on full bytes (which are 8-bits each), so we can only use code lengths of 8, 16, 24 and 32 bits. With this knowledge, we can sacrifice some bits for the purpose of storing code length in them. Letâ€™s first take a look at the reference table, and Iâ€™ll follow with an explanation.

Letâ€™s focus on â€œByte 1â€ column. We can see that depending on a number of bytes we use, the code begins differently. 4 bytes code starts with 4 ones, 3 bytes with 3 ones, 2 bytes with 2 ones, and 1 byte is different.

Anyway, as you might have guessed, the first byte tells us how many bytes should be read as one character. If you see something beginning with â€œ1110â€¦â€ you know that you need to read two more bytes to get a whole character. And as you can see those â€œnextâ€ bytes will all begin with â€œ10â€¦â€. If you start reading a character and it begins with â€œ10â€ then you instantly know something is not right. But you already know how it should begin, so you can skip a few bytes until you find one starting with anything other than â€œ10â€¦â€. And thatâ€™s a great feature of this encoding, as it allows easy and fast processing and error recovery (you know, data get lost in the Internet sometimes).

However, you may be worried, that weâ€™re losing some precious space by sacrificing bits of information for such things. And we are indeed. The fixed-length 32-bit code could theoretically encode 4 billion characters. But as we see in the table above, UTF-8 only gives you 21 bits for code point. Anyway, itâ€™s still over 2 million characters, enough to meet Unicode standard (which defines a maximum of 1112064 characters). The limiting factor here is backwards compatibility with UTF-16, which is the oldest of Unicode encodings.

But even those 1,1 million characters is more than enough. Unicode 9.0 uses just 11% of all available space (128237 characters to be exact), and weâ€™ve already encoded pretty much all available languages and scripts including Egyptian Hieroglyphs, Domino and Mahjong tiles, and of course Emojisâ€Šâ€”â€Šthe language of the 21st century ğŸ‘ŒğŸ˜‚

But why is one byte code different from the others? Why didnâ€™t they do it all nice and satisfying?

People creating UTF-8 wanted it to be as easy to use as possible. If most encodings were based on ASCII, then it would be only reasonable to make it ASCII-compatible as well. The way they did it makes all ASCII encoded texts also UTF-8 encoded. This made switching to Unicode much easier and is one of the great things about this encoding.

Enough with the theory. Letâ€™s see it in practice.

The good start to lay base ground for required calculations is trying to encode something from ASCII, because itâ€™s supposed to fit into one byte.

For example, letâ€™s take a letter â€œDâ€. If we check the ASCII table Iâ€™ve posted in the previous post, we can check that itâ€™s code is 68, which is ox44 in base-16 (hexadecimal codes are actually used more commonly, so weâ€™ll stick with them). Now we should check Unicode table to find out that â€œDâ€ is U+0044. â€œU+â€ is a prefix for Unicode codes, and the rest is a hexadecimal value. If you know how to convert from base-16 into binary, you know that 0x44 = 01000100. Otherwise just believe my words.

So, we already know the code for â€œDâ€ in ASCII. And if we look at the table Iâ€™ve posted above, we can see that U+0044 is lower than U+007F, so this letter fits into one byte. So the code for â€œDâ€ in Unicode is also 01000100. Just as expected.

We can now do the same for letters â€œoâ€ and â€œgâ€ to encode the word â€œDogâ€. It will be the same in both ASCII and Unicode: 01000100 01101111 01100111. Notice how each letter starts with a zeroâ€Šâ€”â€Šthis is an easy way to tell weâ€™re dealing with ASCII here (which is equivalent to 1-bit UTF-8).

But letâ€™s be serious. Itâ€™s 2017 and nobody writes â€œDogâ€ anymore. Nowadays we use a dog emoji â€œğŸ¶â€ instead. Emojis didnâ€™t exist in ASCII so we go straight to Unicode table to find itâ€™s code: U+1F436.

Looking at the conversion table, weâ€™ll need all 4 bits to encode it, because U+1F436 is bigger than U+10000. After converting to binary we get 000011111010000110110. There are 4 zeros in the beginning to make it have a required length of 21 bits.

We can now change â€œxâ€ in the conversion table into our binary digits and get an UTF-8 code for â€œğŸ¶â€: 11110000 10011111 10010000 10110110. Numbers written in bold are those, weâ€™ve calculated above.

As weâ€™ve expected. No single byte starts with a zero, and all bytes except for the first begin with â€œ10â€.

If we look at what weâ€™ve already learned it all looks relatively simple and well thought. After all, weâ€™ve just manually encoded some data. Why would there be any problems with it? There are three reasons why.

The first one goes all the way back to 1960â€™s when the ASCII was created. Since it became the de-facto standard base for all future encodings, there was no reason for Unicode to break it. So it was created to be compatible with ASCII. And it was a nice little thing. But why limit compatibility to ASCII alone? We can try to make it overlap with as many encodings as possible. And it usually went quite well, but there were a few exceptions.

Do you know what character had a 0xB5 (decimal 181) code in standard Extended ASCII? It was Âµâ€Šâ€”â€Ša symbol for micro. And do you know what is the 12th letter of the Greek alphabet? It is of course Î¼â€Šâ€”â€Šthe same character. Or is it?

You see, the first character (weâ€™ll call it â€œmicroâ€) was kept as U+00B5 to make Unicode and older encodings work together nicely. It looks the same, and actually comes from the greek letter Î¼ (which is called â€œmuâ€), but it has a different meaning. Since Unicode doesnâ€™t group characters by their appearance, but rather divides them into groups of the same origin and context. This means that there is a letter Î¼ again under U+03BC, where all the greek letters are.

This is of course well defined in Unicode standard, and the micro character is marked as â€œcompatibilityâ€. It is recommended to use small greek Î¼ instead, but try and tell it to people using your application.

The second one goes back about 8000 years back. Yes, itâ€™s no mistake here. Human languages and writing systems themselves are probably one of the hardest things to standardise. There are thousands of languages, using hundreds of writing systems. Through history, many languages have diverged from others making them similar but not quite the same. Writing systems often evolved one from another making them share some symbols, but not all of them.

Time for an example. Letâ€™s say youâ€™re going to visit the Netherlands. You board a plane, and after a few hours, you arrive in Amsterdam. We all know its name begins with the letter â€œAâ€â€Šâ€”â€Šthe first letter of an alphabet. But when you look at the map you can find many other cities. For example, Astana which is the capital city of Kazakhstan. Or if you have a map which happens to be in Russian look for â€œĞÑÑ‚Ğ°Ğ½Ğ°â€. This first Cyrillic letter looks a lot like a letter â€œAâ€, doesnâ€™t it? But we know it canâ€™t be the same letterâ€Šâ€”â€Šafter all, itâ€™s from a language using a different alphabet. Or you may find yourself in sunny Athens, the capital of Greece. But if youâ€™ve bought a Greek map, youâ€™re looking for â€œÎ‘Î¸Î®Î½Î±â€. And again, this first letter looks like â€œAâ€, but we know itâ€™s a greek capital letter alpha.|||

Last time, we went all the way from the Morse Code to the creation of Unicode. Now weâ€™ll learn how Unicode works under the hood.