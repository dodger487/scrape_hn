Want to see something weird yet amazing?

Go ahead and open a new tab and type â€˜donald trumpâ€™ into your Google.

Alright, how many hits did you get and how long did it take?

406,000,000Â results in just 0.92 seconds? (donâ€™t worry if yourâ€™s is slightly different, as it varies each second)

Great, now type in your name into Google.

How many hits did you get and how long did it take?

Why is it thatÂ when you googled â€˜donald trumpâ€™ did you get more hits (web content) in just 0.92 seconds, whilst when you googled your own name you found it had less hits but it seemed to take longer to load up so little hits compared to when you googled â€˜donald trumpâ€™?

That my friend is the work of your web crawlersâ€¦

Think of them as kind of like virtual Wall-Es

Except rather than they scraping and looking for junk, it is these virtual crawlersâ€™ destiny to explore around the whole World Wide Web seeking all forms of information.

Mostly it comes down to these two kinds of information:

For example, all the images, videos and text that you see here on this page.

Basically all links going out from this page to other pages on this website or other websites.

Thatâ€™s what these little robots do.

Say if we took Saint as the website that you want to crawl, so you type the URL of the homepage into the spider() function and then what happens is, it looks at all the content on this website.

Now this robot doesnâ€™t really view how both you and I view any of the multimedia (videos, pictures, etc.) on this website, instead it just looks at the â€œtext/htmlâ€ as described in the code of the videos and pictures.

So letâ€™s say now you want the little robot to crawl and find the word â€˜socksâ€™ on this page.

What you do is, you add into your code that you want the little guy to crawl and look for the word â€˜socksâ€™ on this page.

If the word â€˜socksâ€™ is not found in the text on this page, the robot will move on to the next link in its collection (it can be any site, it doesnâ€™t have to be only this site) and it repeats the process again and again

It continues to look, until the robot has either found the word â€˜socksâ€™ or runs into the limit that you typed into the spider() function.

Google has a whole army of these little guys, and they are the ones who crawl all over the web looking for new content.

Now I have a question specially for you,

How would you keep what is already crawled, faster to be seen by people?

If you said increase the number of web crawlers, then that wouldnâ€™t do much because thereâ€™s already so many web crawlers (itâ€™s more than you can even imagine) crawling the web as you read this word this very second, and it would be counterproductive since they still need time to find your content and crawl it entirely.

If you said seeding like how torrents seed by donating some of your downloaded data for others to use to access the same information.

Well thatâ€™s a great idea but that is just going take up more of your own internet which then leads to even more of your time being taken for the search results to load up wouldnâ€™t it?

So how you would you tackle this problem?

Like a library, Google stores whatever that is crawled into its own library and this is where Indexing comes into play.

If itâ€™s not for Indexing, the web crawlers will have to crawl every time and it will take ages for you to find what youâ€™re looking for on Google.

Now this is why,

Whenever you google â€˜donald trumpâ€™ you can get like 406,000,000Â results in just 0.92 seconds compared to when you searched for your own name.

The more content that is crawled and indexed to the ever growing library of Google, the faster it can be accessed, which is why when you googled your own name, it took more time because the web crawlers havenâ€™t gotten to meet you yet. ğŸ™‚

*Your search terms also visit a number of databases simultaneously such as spell checkers, translation services, analytic and tracking servers, etc but again indexing plays a central role in how fast you can view crawled content online compared to whatâ€™s being crawled this very second.

I have tried the following code a few days ago on my Python 3.6.1 (which is the latest as of 21st March 2017) and it should work for you too.

Just go ahead and copy+paste this into your Python IDE, then you can run it or modify it.|||

Want to see something amazing? Type into your Google this... Here's a quick and simple guide on how you can make a web crawler in under 50 lines of code.