In an era of ongoing attacks and surveillance, proper generation of random numbers is essential to the security of our systems and communications, so the quality of our random numbers is often a topic of discussion. The performance with which the kernel comes up with random numbers is not normally as much of an issue. It turns out, though, that, on large NUMA systems with heavy demand for random numbers, lock contention within the random-number generator (RNG) can severely limit the performance of the system as a whole. A patch addressing that problem is relatively straightforward, but it provides an opportunity to look at how this subsystem works in general.

Most readers will be familiar with the fact that the kernel's RNG subsystem collects entropy (randomness) and provides it via two interfaces. One, exposed as , is strictly limited so that it cannot provide more entropy than has been collected by the system; the other ( ) functions as a pseudo-random-number generator to be able to continue to provide random data when the supply of incoming entropy is not sufficient to meet the demand. For most applications, even cryptographic applications, the latter interface is more than sufficient, but is there for those who need truly random data and are able to wait for it if need be.

These interfaces are supported by three "entropy pools" within the kernel; an entropy pool is an array of bytes of random data, along with some supporting metadata. Whenever randomness is collected by the kernel (be it from interrupt timing, a hardware RNG, or some other source), it is added to the input pool, which contains 4096 bits of data. The pool is not a simple FIFO of random bytes; instead, randomness is "mixed" into the pool with an algorithm that resembles a CRC calculation. The mixing is meant to be fast (so it can be done from interrupt handlers) and to spread the available entropy through the entire pool. It is also intended to keep the state of the pool from being known, even if the attacker is able to write a large amount of known data into it.

The kernel maintains an estimate of the amount of entropy stored in the pool at any given time. That estimate increases when randomness is added to the pool (by an amount that depends on an estimate of how random the input data truly is), and it is decreased when entropy is removed from the pool.

Since the pool is not a FIFO, one does not simply read random bytes out of it. Instead, entropy is extracted by calculating an SHA-1 hash of the pool. The hashed value is returned as the requested random data, but it is also mixed back into the pool. Using the hash will, once again, help to keep the state of the pool from being known.

Users of random data do not simply read it from the input pool, though; instead, the kernel maintains a simple hierarchy of three pools:

Reads from will extract data from the blocking pool, while reads from use the non-blocking pool. The output pools are smaller, each holding a maximum of 1024 bits of entropy in the 4.4 kernel. Entropy spills from the input pool into the two output pools in two ways:

Many years ago, data was read from the output pools without locking; perhaps the potential for corruption of random data was not seen as being particularly worrisome. But it turned out that, on occasion, it was possible for two processes to read the same random bytes, a vulnerability that could make it possible for one process to know which random numbers were being used by another. So a spinlock was added to each pool to ensure that access to the pools is properly serialized. It is that locking that turns out to be a bottleneck if too many processes (on a large number of CPUs) are trying to read random data at the same time.

After running into the problem, Andi Kleen put together a patch set designed to alleviate this lock contention. It uses the classic approach of avoiding inter-CPU contention by giving each CPU (or, properly in this case, each NUMA node) its own data. To get there, Andi's patch modifies the pool structure to look like this:

In short: each NUMA node gets its own non-blocking pool to read from, so reading random data no longer requires cross-node locking. Each (along with the blocking pool) receives overflow from the input pool in a round-robin fashion, and each can draw from that pool in response to a request if entropy is available. There is, Andi says, no need for per-CPU pools at this time, though things could be split further in the future if that need were to arise. There is also no plan to make a per-node version of the blocking pool; code that is willing to wait for sufficient entropy is unlikely to have trouble with locking scalability.

The patch does indeed result in increased scalability for non-blocking random-number generation on large systems. It also has the effect of distributing the entropy pool across nodes, making it that much harder to guess the state of the pool as a whole. One potential disadvantage is that it is no longer possible to read out the state of all output pools at system shutdown time, meaning that some entropy may be lost over a reboot. That could be fixed with the addition of a new save/restore interface, but it is not clear that anybody is concerned enough to do that work.

This patch has been through a set of revisions in response to comments and seems likely to be ready for merging into the 4.5 kernel.|||

