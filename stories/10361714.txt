In the first article weâ€™ve learned a bit about Data ScienceÂ for Losers. And the most important message, in my opinion, isÂ thatÂ patterns are everywhere but many of themÂ canâ€™t be immediately recognized. This is one of the reasons why weâ€™re digging deep holes in our databases, data warehouses, and other silos. In this article weâ€™ll use a few more methods from Pandasâ€™ DataFrames and generate plots. Weâ€™ll also createÂ pivot tables and queryÂ an MS SQL database via ODBC. SqlAlchemy will be our helper in this case and weâ€™ll see that even Losers like us can easilyÂ merge and filter SQL tables without touching the SQL syntax. No matter the task you always need a powerful tool-set in the first place. Like theÂ Anaconda DistributionÂ which weâ€™ll be using here. Our data sources will be things like JSON files containing reddit comments orÂ SQL-databases likeÂ Northwind. Many 90â€™es kids used Northwind to learn SQL.Â  ğŸ˜

The notebook from this article can be foundÂ here.

Our first analysis will use 10.000 comments from a reddit backup. More specifically, we will use 10.000 commentsÂ from August 2015. There are many more gigabytes of redditâ€™s backups available, so feel free to download entries from other time periods. If you prefer Torrent hereâ€™s the link. You can also query these data sets via browser by using Google BigQuery. Â In the end you should have a JSON file containing many entries which weâ€™ll try to load into a DataFrame. But as weâ€™ve already learned most data is not in the format we expect it to be. Sooner or later weâ€™ll have to structurally modifyÂ our data. Hereâ€™s a snippet of ourÂ reddit JSON:

Well, this isnâ€™t a proper JSON and Pandas would immediately reject it. Letâ€™s just try to load it just to see what happens.

Well, notÂ a good impression. Pandas sees some trailing data which by itself is misleading enough. But when a machine complains wrongly why should be expect our data to be correct anyway?Â We have to accept the fact that these entries only seeminglyÂ look like a JSON structure but surely donâ€™t follow the JSON standard itself. A collection of JSON objects, that is: reddit comments, has to be inside an JSON array (square brackets, []) and must be separated by commas. Do we see any commas in the document? Yes, but only inside the entries themselves and not between them. And whereâ€™s the array comprising of all reddit comments? No square brackets in sight. So, what areÂ we goingÂ to do now? Converting raw data into a readable format, of course. In this case weâ€™ll use a simple Python snippet to read the comments one by one, put a comma in between, remove any newlines and put the comments into a new array.

Now we can load all comments into a DataFrame but we also have to give Pandas a little more info than usual. This is because itÂ doesnâ€™t know in advance what the internal JSON-structure looks like. Is it a flat array containing only single entries? Or a dictionary? Or something like objects with properties? Therefore it isnâ€™t enough just to type in pd.read_json and expect it to load the unknown structure somehow.Â Instead we have to type in a command like this:

Pandas can load JSONâ€™s either as Series or as DataFrames depending on the internal structure of the document. In our case we want to generate Tables and therefore we want to receiveÂ a DataFrame. For this we need a proper orientation of our data. Therefore we instruct Pandas to use the column orientation. Finally, we want a certain column containing timestamps to be parsed as aÂ date entry. Finally, we check the basic properties of our new DataFrame:

Now letâ€™s create a horizontal bar-plot showing the highest rated comments for all available sub-reddits:

Nice, how quick weâ€™ve got from an unreadable JSON to a plot! And now for something completely different: pivot tables.

Nice, but whatâ€™s a pivot table by definition? Honestly, we could fill at least a book about it but let it be said that a pivot table is an automatic data summarization, grouping, filtering and counting tool.Â  Pivot tables visualize data that automatically adjust itself according to given rules, dimension and filters. For example we decide that it should show the controversality column of every comment.

What happens here is that for the two given indices author & subr there will be a column controversiality. But this selection is not very useful because our index starts with author. There are many authors and much less sub-reddits. Letâ€™s change the order and look at the outcome:

This now is something completely different because our authors are grouped by subreddits. It makes a lot more sense because we can expect many authors to visit different sub-reddits and not only one. Also we have to take into account that the many zero-values are not there because JSON file contained them but because weâ€™ve used the option fill_values=0. Without it the pivot would be filled with NaNâ€™s. The last parameter margins=True calculates the â€œtotalsâ€. There are many more options, of course. For example, we can instruct Pandas to use certain aggregate functions on every field or only some of them. Here we want Pandas to calculate sums:

And suddenly, our pivot table looks a lot nicer. No more useless zeroes all along the way. We can add more than just one aggregate function. Itâ€™s even possible to create a dictionary of aggregate functions. The column would be the key-entry inÂ the dictionary and the function its corresponding value. Here we let Pandas calculate mean-values:

OK, the outcome is pretty silly, but you get the point. Itâ€™s a Loserâ€™s way to a higher knowledge, anyway.

A pivot table can be used like any other data source, too. You can execute queries against it and do all the nice filtering stuff as if it was a database.

Again, SQLAlchemy is something that surely deserves a book or two and therefore Iâ€™m not going to talk too much about it because it simply doesnâ€™t fit into a simple tutorial like this. In our case all we have to know is that SQLAlchemy is a powerful Python package givingÂ us the possibility to query any kind of database without touching its vendorâ€™s specifics. For example, here Iâ€™m using an MS SQL Server (just to annoy the most of you) and access it via ODBC (yes, still annoying you!). The ODBC setup can be a little bit daunting so let me quickly explain how to do it under Windows:

OK, hopefully it wasnâ€™t too annoying.Â  ğŸ™„

We now have to configure SQLAlchemy to talk to our new ODBC resource:

Here we see many other â€œstandardâ€ imports like those with Pandas, NumPy etc. But at the bottom we additionally configure SQLAlchemy. The pyODBC package is needed too, because SQLAlchemy uses it to create an ODBC context.

SQLAlchemy offers the method create_engine which we have to feed with a special connection string containing the name of the database driver, in this case mssql+pyodbc, the database catalog information and its access tokens. The result of this call will be a reference to an engine object maintaining a connection to the given database catalog. This is how the structure of the NORTHWIND catalog looks like:

First, weÂ let SQLAlchemy access certain tables from the catalog:

We access tables withÂ SQLAlchemy byÂ providing their respective names. The second parameter,Â engine, maintains theÂ physical connectionÂ against theÂ catalog. The resulting references are just normal DataFrames which is of course an important advantage of Pandas. No matter what type the original data source isÂ in the end everything ends up in a DataFrame. Or Series if weâ€™re not maintaining multidimensional structures.

We can now execute methods that resemble behaviors known from ordinary databases. For example,Â Table JoinsÂ via merge:

Here we instruct Pandas to merge two tables by using certainÂ primary keys from bothÂ whenÂ combining their rows into a new table. The parameter how instructs Pandas to use the inner-join which means it will only combine such rows which belong to both of the tables. Therefore weâ€™ll not receiveÂ any NaN-rows. But in some cases this could be desirable. Then use the alternative options like left, right or outer.

And of course itâ€™s possible to generate the same pivot tables with data that came from SQLAlchemy. Theyâ€™re nothing else but DataFrames all the way down. OK, not absolutely all the way down, because there are also Series and NumPy arrays etc., but this is a little bit too much of knowledge for Losers like us. Maybe in some later articles.

Here we let an aggregate sum-function from NumPy be only executed on Quantity fields. Being armored with some knowledge from the previous pivot example it shouldnâ€™t be that hard to create own pivotâ€™s. And of course, we can query pivot tables, too.Â  ğŸ˜|||

data science tutorial