When I came across the quote above by R. A. Fisher while reading a paper on probability theory,Â IÂ couldnâ€™t help butÂ think about the vast corpus ofÂ documents that contend for survival (relevance) for a search query. To add to that the ever increasing bar of expectations of the results to be in the top 10.Â Â On average, 71.33% of searches resulted in a page one Google organic click. Page two and three get only 5.59% of the clicks. On the first page alone, the first 5 results account for 67.60% of all the clicks and the results from 6 to 10 account for only 3.73%. As you can see in the chartÂ below:

It becomes almost imperativeÂ toÂ think in terms ofÂ fitnessÂ function that is required to survive the naturalÂ selection of organic search results. Â There are many ways toÂ define aÂ fitness function; After a few hours ofÂ reading throughÂ Manning, getting someÂ Math squared out atÂ Khan Academy and googling around;Â I settledÂ on choosing threeÂ of (many) standard ways to measure relevance of information retrieval; AsÂ listed below, lets lookÂ intoÂ these in some detail:

nDCG (Normalized Discounted Cumulative Gain) â€“ nDCG or Normalized Discounted Cumulative Gain is a way to measure the quality of a group of search results. It is based on the following hypotheses:

OK, Time for someÂ Math ! Lets consider a hypothetical set of 10 search results. Going from top to bottom and ranking each as follows:

(0 â€“ Not Relevant, 1- Somewhat relevant, 2- Relevant, 3- Very Relevant) â€“ Fair enough?

So according to the mathematical definition of cumulative gain, we need to sum up the relevancy values for the 10 results as follows:

So say hypothetically say if your relevance score for the top 10 in order of results top to bottom were (3, 2, 3, 0, 0, 1, 2, 2, 3, 0), youÂ would end up with a CG of 16. But as you may have noticed that the second last result here has a Relevance score of 3 (Very Relevant) whereas the 4thÂ is 0 (not relevant) â€¦ CG(cumulative gain) does not factor in the position of results which is important as weÂ discussed in ourÂ hypotheses above. We need to penalize these highly relevant documents appearing lower in the search results.Â So effectively gain isÂ accumulated starting at the top of theÂ ranking and may be reduced, or discounted, atÂ lower ranks.Â A typical way to discount is to reduce the graded relevancy logarithmically.Â Typical discount is 1/log (rank).With base 2, the discount at rank 4 is 1/2, and at rank 8 it is 1/3 etc..

SoÂ Discounted Cumulative Gain(DCG) is theÂ is the total gain accumulated at a particular rank p.

Following our sample data above:

Now although DCG isÂ getting is closer to where we want to get ,Â there is still our 3rd conjecture, â€œNot all queries are equal!â€ which means that some are just harder than the others and will produce lower scores Â than the easierÂ queries. To mitigate this fact we need toÂ instrument some form of normalization. To achieve this we need to scale the results based on the best seen result, so essentiallyÂ calculate whats calledÂ iDCG or Ideal DCG. Â EssentiallyÂ done by sorting documents of a result list by relevance, producing the maximum possible DCG till position p, also called Ideal DCG (IDCG) till that position; WhichÂ makes theÂ implementation a bit harder as we need to calculate theÂ Â iDCG before computing the nDCG for any given result.

The nDCG values for all queries can then be averaged to obtain a measure of the performance of a search engineâ€™s ranking.SoÂ going back to our example above theÂ iDCG = 3, subsequently making theÂ nDCGÂ values as : (1, 0.66, 0.63, 0, 0, 0.13, 0.24, 0.22, 0.32, 0)

ThusÂ nDCGÂ is then relative values on the interval 0.0 to 1.0 and can be scaled across queries to determine relevance.

Taking this for a test drive with a few sampleÂ queries across Google and Bing, was a good exercise in validating the equationsÂ above, and needless to mention which one was more relevantÂ  ğŸ™‚ . I am quite anxious to try this out in the Enterprise Search world where the dynamicsÂ are a bit different. (something for the next post). In the next post we will also look at other functions such as F-measure and MRR (Mean Reciprocal Rank) to measure search relevance.

There is a quote I like â€œNatural selection is anything but random.â€œ; Defining and validating fitness functionsÂ that enable us to measure and account for relevanceÂ is critical factor inÂ measuring and optimizing for survival.|||

Natural selection is a mechanism for generating an exceedingly high degree of improbability. When I came across the quote above by R. A. Fisher while reading a paper on probability theory,Â IÂ couldn't help butÂ think about the vast corpus ofÂ documents that contend for survival (relevance) for a search query. To add to that the ever increasing barâ€¦