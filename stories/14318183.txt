A few months ago, I installed a Chrome extension called â€œShow Facebook Computer Vision Tags.â€ It does exactly what it says on the tin. Once installed, images on my feed were overlaid with one or many emojied descriptors revealing the â€œaltâ€ tags that Facebook automatically adds to an image (using a â€œDeep ConvNet built by Facebookâ€™s FAIR teamâ€) when it is uploaded. This feature, which the company launched in 2016, is meant as a tool for the visually impaired, who often rely on text-to-voice screen-readers. With these tags added, the screen reader will narrate: â€œImage may contain: two people, smiling, sunglasses, sky, outdoor, water.â€ The user may not be able to see an image, but they can get an idea of what it contains, whether people are wearing accessories or facial hair, when theyâ€™re on stage or playing musical instruments, whether theyâ€™re enjoying themselves. The tags, in fact, only note positive emoting: smiling or not.

This seems a remarkably limited subset of linguistic and conditional terms for a platform of Facebookâ€™s ubiquity, especially given its investment in having images go viral. If virality is predicated upon images that inspire extremes of emotional response â€” the pet that faithfully waits for its dead master; a chemical attack in Syria â€” wouldnâ€™t the tags follow suit? Despite Facebookâ€™s track record of studying emotional manipulation, its tagging AI seems to presume no Wow, Sad, or Angry â€” no forced smiles on vacation or imposter syndrome at the lit party.

A white paper on Facebookâ€™s research site explains that these tags â€” â€œconcepts,â€ in their parlance â€” were chosen based on their physical prominence on photos, as well as the algorithmâ€™s ability to accurately recognize them. Out were filtered the concept candidates that carried â€œdisputed definitionsâ€ or were â€œhard to define visuallyâ€: gender identification, context-dependent adjectives (â€œred, young, happyâ€), or concepts that are too challenging for an algorithm to learn or distinguish, such as a â€œlandmarkâ€ from general, non-landmark buildings. But speaking with New York Magazine earlier this year, Adam Geitgey, who developed the Chrome extension, suggested its training has expanded beyond that: â€œWhen Facebook launched [alt tags] in April, they could detect 100 keywords, but this kind of system grows as they get more data â€¦ A year or two from now they could detect thousands of different things. My testing with this shows theyâ€™re well beyond 100 keywords already.â€ As such, the Chrome extension is less interested in Facebookâ€™s accessibility initiatives, instead aiming to draw attention to the pervasiveness of data mining. â€œI think a lot of internet users donâ€™t realize the amount of information that is now routinely extracted from photographs,â€ Geitgey explains on the extensionâ€™s page in the Chrome web store, â€œthe goal is simply to make everyone aware.â€

As the use of emojis makes plain, the extension addresses sighted users â€” those who do not use screen readers and are probably unaware, as I was, of this metadata Facebook adds to images. With the extension, a misty panorama taken from the top of the worldâ€™s tallest building becomes â˜€ï¸ ï¸for sky, ğŸŒŠ ocean, ğŸš´ outdoor,ğŸ’§water; a restaurant snap from Athens, meanwhile, is ğŸ‘¥ six people, ğŸ˜‚ people smiling, ğŸ´ people eating, ğŸ food, and ğŸ  indoor. (Sometimes, when thereâ€™s no corresponding emoji it adds an asemic â–¡.) The tags arenâ€™t always completely right, of course; sometimes the algorithms that drive the automatic tagging misses things: recognizing only one person where there are three in a boat in Phuket Province, describing a bare foot as being shod. But thereâ€™s something attractive in its very prosaic reduction of an image down to its major components, or even its patterning, as with an Alex Dodge painting of an elephant that is identified only as â€œstripes.â€ The automatic tagging doesnâ€™t seem integrated with Facebookâ€™s facial recognition feature (â€œWant to tag yourself?â€) but rather allows you to view your life and the lives of your friends as a stranger might, stripped of any familiar names, any emotional context that makes an image more than the sum of its visual parts â€” resplendent in its utter banality.

Perhaps itâ€™s a legacy of growing up in the UAE, where you can fully expect every click, scroll, or even sneeze in a public space to be recorded, but Iâ€™m not bothered to find that according to its algorithmically generated â€œpreferencesâ€ page in my profile, Facebook thinks my interests include â€œprotein-protein interaction,â€ â€œfirst epistle to the Thessalonians,â€ and caviar (Iâ€™m a vegetarian); that it considers me both an early technology adopter and a late one.

Infinitely more exciting is the transposed comic-book dream of X-ray vision â€” seeing through the image to what the machine sees. I want to be able to access that invisible layer of machine-readable markup to test my vision against a computerâ€™s. The sentiment is not that different from the desire to see through the eyes of the other that has historically manifested itself in the colonial history of anthropology or in texts like John Howard Griffinâ€™s Black Like Me. The desire to see what they see, be it other people or machines, is a desire to feel what they feel. AI researcher Eliezer Yudkowsky described the feeling of intuition as the way our â€œcognitive algorithms happen to look from the inside.â€ An intangibly human gut response is just as socialized (programmed) as anything an algorithm might â€œfeelâ€ on the inside, clinging to its intuitions as well. It should be enough to take the algorithmsâ€™ output at face value, the preferences they ascribe to me, or to trust that it is the best entity to relay its own experience. But Iâ€™m greedy; I want to know more. What does it see when it looks at me?

The American painter and sculptor Ellsworth Kelly â€” remembered mainly for his contributions to minimalism, Color Field, and Hard-edge painting â€” was also a prodigious birdwatcher. â€œIâ€™ve always been a colorist, I think,â€ he said in 2013. â€œI started when I was very young, being a birdwatcher, fascinated by the bird colors.â€ In the introduction to his monograph, published by Phaidon shortly before his death in 2015, he writes, â€œI remember vividly the first time I saw a Redstart, a small black bird with a few very bright red marks â€¦ I believe my early interest in nature taught me how to â€˜see.â€™â€

Vladimir Nabokov, the worldâ€™s most famous lepidopterist, classified, described, and named multiple butterfly species, reproducing their anatomy and characteristics in thousands of drawings and letters. â€œFew things have I known in the way of emotion or appetite, ambition or achievement, that could surpass in richness and strength the excitement of entomological exploration,â€ he wrote. Tom Bradley suggests that Nabokov suffered from the same â€œreferential maniaâ€ as the afflicted son in his story â€œSigns and Symbols,â€ imagining that â€œeverything happening around him is a veiled reference to his personality and existenceâ€ (as evidenced by Nabokovâ€™s own â€œentomological eruditionâ€ and the influence of a most major input: â€œAfter reading Gogol,â€ he once wrote, â€œoneâ€™s eyes become Gogolized. One is apt to see bits of his world in the most unexpected placesâ€).

For me, a kind of referential mania of things unnamed began with fabric swatches culled from Alibaba and fine suiting websites, with their wonderfully zoomed images that give you a sense of a particular materialâ€™s grain or flow. The sumptuous decadence of velvets and velours that suggest the gloved armatures of state power, and their botanical analogue, mosses and plant lichens. Industrial materials too: the seductive artifice of Gore-Tex and other thermo-regulating meshes, weather-palimpsested blue tarpaulins and piney green garden netting (winningly known as â€œshade clothâ€). What began as an urge to collect colors and textures, to collect moods, quickly expanded into the delicious world of carnivorous plants and bugs â€” mantises exhibit a particularly pleasing biomimicry â€” and deep-sea aphotic creatures, which rewardingly incorporate a further dimension of movement. Walls suggest piled textiles, and plastics the murky translucence of jellyfish, and in every bag of steaming city garbage I now smell a corpse flower.

â€œThe most pleasurable thing in the world, for me,â€ wrote Kelly, â€œis to see something and then translate how I see it.â€ I feel the same way, dosed with a healthy fear of clichÃ© or redundancy. Why would you describe a new executive order as violent when you could compare it to the callous brutality of the peacock shrimp obliterating a crab, or call a dress â€œblueâ€ when it could be cobalt, indigo, cerulean? Or ivory, alabaster, mayonnaise?

We might call this impulse building visual acuity, or simply learning how to see, the seeing that John Berger describes as preceding even words, and then again as completely renewed after he underwent the â€œminor miracleâ€ of cataract surgery: â€œYour eyes begin to re-remember first times,â€ he wrote in the illustrated Cataract, â€œâ€¦details â€” the exact gray of the sky in a certain direction, the way a knuckle creases when a hand is relaxed, the slope of a green field on the far side of a house, such details reassume a forgotten significance.â€ We might also consider it as training our own visual recognition algorithms and taking note of visual or affective relationships between images: building up our datasets. For myself, I forget peopleâ€™s faces with ease but never seem to forget an image I have seen on the internet.

At some level, this training is no different from Facebookâ€™s algorithm learning based on the images we upload. Unlike Google, which relies on humans solving CAPTCHAs to help train its AI, Facebookâ€™s automatic generation of alt tags pays dividends in speed as well as privacy. Still, the accessibility context in which the tags are deployed limits what the machines currently tell us about what they see: Facebookâ€™s researchers are trying to â€œunderstand and mitigate the cost of algorithmic failures,â€ according to the aforementioned white paper, as when, for example, humans were misidentified as gorillas and blind users were led to then comment inappropriately. â€œTo address these issues,â€ the paper states, â€œwe designed our system to show only object tags with very high confidence.â€ â€œPeople smilingâ€ is less ambiguous and more anodyne than happy people, or people crying.

So there is a gap between what the algorithm sees (analyzes) and says (populates an imageâ€™s alt text with). Even though it might only be authorized to tell us that a picture is taken outside, then, itâ€™s fair to assume that computer vision is training itself to distinguish gesture, or the various colors and textures of the slope of a green field. A tag of â€œskyâ€ today might be â€œcloudy with a threat of rainâ€ by next year. But machine vision has the potential to do more than merely to confirm what humans see. It is learning to see something different that doesnâ€™t reproduce human biases and uncover emotional timbres that are machinic. On Facebookâ€™s platforms (including Instagram, Messenger, and WhatsApp) alone, over two billion images are shared every day: the monolithâ€™s referential mania looks more like fact than delusion.

Within the fields of conservation and art history, technology has long been deployed to enable us to see things the naked eye cannot. X-ray and infrared reflectology used to authenticate forgeries, can reveal, in a rudimentary sense, shadowy spectral forms of figures drafted in the original composition, or original paintings that were later covered up with something entirely different, like the mysterious bowtied thinker under Picassoâ€™s early 1901 work, Blue Room, or the peasant woman overpainted with a grassy meadow of Van Goghâ€™s 1887 work Field of Grass, or the racist joke discovered underneath Kazimir Malevichâ€™s 1915 painting Black Square, suggesting a Suprematism underwritten by white supremacy.

But what else can an algorithm see? Given the exponential online proliferation of images of contemporary art, to say nothing of the myriad other forms of human or machine-generated images, itâ€™s not surprising that two computer scientists at Lawrence Technical University began to think about the possibility of a computational art criticism in the vein of computational linguistics. In 2011, Lior Shamir and Jane Tarakhovsky published a paper investigating whether computers can understand art. Which is to say, can they sort images, posit interrelations, and create a taxonomy that parallels what an academic might create? They fed an algorithm around a thousand paintings by 34 artists and found that the network of relationships it generated â€” through visual analysis alone â€” very closely matched what has come to be canonized as art history. It was able, for example, to clearly distinguish between realism and abstraction, even if it lacked the appropriate labels: what we today call classical realism and modernism it might identify only as Group A and Group B. Further, it broadly identified sub-clusters of similar painters: Vermeer, Rubens, and Rembrandt (â€œBaroque,â€ or â€œA-1â€ perhaps); Leonardo Da Vinci, Michelangelo, and Raphael (â€œHigh Renaissanceâ€ or â€œA-2â€); Salvador DalÃ­, Giorgio de Chirico, Max Ernst (â€œSurrealism,â€ â€œB-1â€); Gaugin and CÃ©zanne (â€œPost-Impressionism,â€ â€œB-2â€).

When looking at a painting, an art historian might consider the formal elements of line, shape, form, tone, texture, pattern, color and composition, along with other primary and secondary sources. The algorithmâ€™s approach is not dissimilar, albeit markedly more quantitative. As an Economist article called â€œPainting by Numbersâ€ explains, Shamirâ€™s program

While the algorithm reliably reiterated what art historians have come to agree on, it went even further, positing unexpected links between artists. Paraphrasing Shamir, the Economist article suggests that Vincent Van Gogh and Jackson Pollock, for example, exhibit a â€œshared preference for low-level textures and shapes, and similarities in the ways they employed lines and edges.â€ While the outcomes are quite different, the implication is that the two artists employed similar painting methods on a physical level, not immediately visually discernible. Were they both slightly double jointed to the same degree? Did they both have especially short thumbs that made them hold the brush a certain way?

Whether Pollock was actually â€œinfluencedâ€ by Van Gogh â€” by mere sight or by private rigorous engagement; in the manner of clinamen, tessera, or osmosis â€” or not at all, Shamirâ€™s AI insisted on patterns and connections that we, art historians, or even Pollock himself, would miss, dismiss or disown.

What the algorithm is doing, noting that â€œthis thing looks like that thing and also like that thing so they must be related,â€ is not unlike what a human would do, if so programmed. But rather than relegate the algorithm to looking for the same correspondences that a human might already see and arrive at the same conclusions, could it go further? Could it produce a taxonomy that takes emotional considerations into account?

An automated Tumblr by artist Matthew Plummer-Fernandez called Novice Art Blogger, generated by custom software run on a Raspberry Pi, offers â€œreviewsâ€ of artworks drawn from Tateâ€™s archive, written by a bot. In a sense, it serves as an extension of what critic Brian Droitcour has called â€œvernacular criticismâ€: â€œan expression of taste that has not been fully calibrated to the tastes cultivated in and by museums.â€ The Tumblr suggests a machine vision predicated not only on visual taxonomies, as with Facebookâ€™s or Shamirâ€™s algorithms but rather one that incorporates an emotional register too â€” that intangible quality that turns â€œbeach, sunset, two people, smilingâ€ into â€œa fond memory of my sisterâ€™s beach wedding.â€ NABâ€™s tone is one of friendly musing, with none of the exclamatory bombast of the more familiar review bots one finds populating comment sections. This one first generates captions and then orders them, rephrasing them in â€œthe tone of an art critic.â€

Take Jules Olitskiâ€™s 1968 gorgeously luminous lilac, dusky pink and celery wash of a painting Instant Loveland: â€œA pair of scissors and a picture of it,â€ NAB says, â€œor then a close up of a black and white photo. Not dissimilar from a black sweater with a blue and white tie.â€ Or John Hoylandâ€™s screenprint Yellows, 1969: I see tangerine and chartreuse squares on dark khaki, the former outlined on two sides in crimson and maroon. NAB, however, sees â€œA picture of a wall and blue sign or rather a person stands in front of a blue wall. Iâ€™m reminded of a person wearing all white leaned up against a wall with a yellow sign.â€ Especially delightful are the earnest little anecdotes it sometimes appends to its reviews, a shy offering. â€œI once observed two birds having sex on top of a roof covered in tileâ€ on Dieter Rothâ€™s Self-Portrait as a Drowning Man, 1974; â€œI was once shown a book, opened up showing the front and back coverâ€ on Richard Longâ€™s River Avon Book, 1979; â€œIt stirs up a memory of a cake in the shape of a suitcaseâ€ on Henry Mooreâ€™s Stringed Figure, 1938/cast 1960. Clearly, itâ€™s not very good at colors or even object recognition â€” perhaps it should consult with @_everybird_ â€” but thereâ€™s still something charming in seeing through its eyes, in being able to feel what it feels.

Eliezer Yudkowsky, in considering the difference between two different neural networks â€” a more chaotic and unpredictable Network 1, wherein all units (texture, color, shape, luminance) of the object it sees are testable, and a â€œmore humanâ€ Network 2, wherein all roads lead to a more central categorization â€” describes their separate intuitions this way:

What the Novice Art Bot doesnâ€™t know is art history. It doesnâ€™t recognize Olitskiâ€™s canvas as an example of Color Field painting, or distinguish between the myriad subgenres of abstraction in contemporary art, but perhaps that doesnâ€™t matter. Itâ€™s Trumpâ€™s America. Maybe it truly is less important to know and more important, instead, to feel.|||

Algorithms don&#8217;t just sort images, they give machines a sense of intuition, the ability to feel what an image means