If you ask academic machine learning experts about the things that annoy them, high up the list is going to be overblown headlines about how machines are beating humans at some task where that is completely untrue. This is partiallyÂ because reality is already so damn amazing there is no need for hyperbole. AlphaGo beat Lee Sedol convincingly. Most of Atari is solved. Professional transcriptionists lose to voice recongition systems.

Object recognition has been counted on the machine side of the tally for yearsÂ (albeit with a few more reservations).

But not medicine. Not yet.

Considering the headlines we see, this may surprise many people. For someone who watches the medical AI space, it seems like a day canâ€™t go by without some new article reporting on a new piece of research in which the journalists say machines are outperforming human doctors. Iâ€™m sure anyone who stumbles on this blog has seen many of them.

Computer Program Beats Doctors at Distinguishing Brain Tumours from Radiation Changes (Neuroscience News, 2016)

Digital Diagnosis: Intelligent Machines Do a Better Job Than HumansÂ (Singularity Hub, 2016)

I didnâ€™t even have to search for these. Almost all of them are still at the top of my Twitter feed.

Now, these are pretty compelling headlines. The second one is even from the Stanford Uni press, not a clickbaitÂ farm. But I hope I can explain why they are both reasonably true statements, but also completely wrong. I think this could be useful for a lot of people, and not just layfolk. Courtesy of Reddit, apparently even some researchers in the field think the machines are already winning.

So I am writing this survival guide: How to read the medical AI reports with a critical eye, and see the truth through the hype.

Because the truth is already amazing and beautiful. We donâ€™t need the varnish.

There are three major ways these articles get it wrong. They either donâ€™t understand medicine, they donâ€™t understand AI, or they donâ€™tÂ actually compare doctors and machines.

The first one is the most important, because this afflicts healthcare technologists as much as journalists. It is also the most common, and therefore the major culprit behind these headlines.

Journalists, technologists, futurists and so on mostlyÂ donâ€™t understand medicine.

Medicine is complex. The biology, the therapeutics, the whole system is so vast it is beyond the scope of any one human mind. Doctors and other healthcare professionals get a feel for it, in a vague and nebulous way, but even that is emphemeral. Some tidbits to remind us how complex treating people actually is:

Medicine is massive.Â Medical research output is larger than any other discipline by orders of magnitude. The scale is mindboggling.

Medicine is idiosyncratic.Â Most of it has grown around a questionable evidence base. Wrong results, misinterpreted results, unreproducible results, no results. Many of our decisions are made for non-scientific reasons, guided by culture, politics, finance, the law. Unless you have been inside it, it is unlikely you will understand it very well. Even from the insideÂ it doesnâ€™t make much sense.

This is explaining why your intuitions about what doctors do are mostly wrong. Just because somethingÂ sounds medical, and seems to be in the scope of medical practice, it doesnâ€™t mean doctors are actually doing it.

And if doctors donâ€™t do it, learn it, get good at it, value it â€¦ is it useful to say that machines are better at it?

Letâ€™s have a look at some examples.

The article I was recommendedÂ in the reddit thread is a pretty famous one. It is a great article from a great research team, and a very valuable contribution that has been taken massively out of context.

It was publicised asÂ Computers trounce pathologists in predicting lung cancer type, severityÂ by the Stanford Medicine News Centre. Fighting talk for sure! Apparently the machine learning system they created has vastly outstripped human pathologists in some sort of predictive task. I am going to ignore the multiple medical errors in the piece, and focus on the meat of the problem. They say computers are better atÂ predicting something about cancer.

This is where the alarm bells should ring. If you see the word â€œpredictiveâ€ in the headline, you can almost stop there.

This is completely unintuitive, but almost always true. Letâ€™sÂ use this article as an example.

So, skilled doctors are pretty much tossing coins in this task. That doesnâ€™t sound right. Maybe we should check the research article itself, maybe the journalists just got it wrong? Here is line two of the abstract.

Is it clear yet?

They trained a computer to identify which patients with cancer will survive for shorter times. That sounds medical, right? It sounds useful, right?

It isnâ€™t. We have no evidence it canÂ help.

Pathologists have trained to provide an answer that will alter treatment choices. Surgery or no surgery. Chemotherapy or radiotherapy. Both, all three, none. These are not the same thing as defining how long someone will live, and there has been no reason to get good at the latter.

Will doing prognosis research help? For sure. Iâ€™m fully on board with the Stanford research team here. This is the future of medicine. Predictive analysis is a great, unbiased way of identifying useful patient groups. It will undoubtedly lead to better treatment decisions. We call this precision medicine. We call it that because it is different than what we currently do. Imprecision medicine, which is built on a whole bunch of compromises and simplifications that work really well despite it all.

The point is that we need a defined idea of what â€œbeating doctorsâ€ actually looks like. If we accept that a machine outperforming a doctor at anything vaguely medicalÂ is enough, we have trivialised the entire concept. Self-driving cars are better than humans at driving without using hands. It verges on tautological.

Prediction isnâ€™t the only place this error in understanding rears its head. Look at the widely reportedÂ Autonomous Robot Surgeon Bests Humans in World First, where a robot â€œoutperformedâ€ human surgeons at suturing (stitching) up a pigâ€™s intestines. Again, amazing work by an amazing team in a very good journal. They created an autonomous bowel-suturing robot. This is a great step forward. In context.

Look at the figure. What did they test? Exactness of suture spacing. How much pressure it took to force the repaired bowel to leak. These are mechanical metrics, and it is unclear how they relate to outcomes. LeakingÂ sounds clinical, but there is no proof that there is a direct relationship between the force needed for leaks, and the number of actual leaks in practice. There could be a threshold effect, and there is no appreciable benefit to â€œbetterâ€ suturing. There could be a sigmoidal pattern, or other more complex relationship. Stronger anastamoses could be worse, stranger things happen in medicineÂ (stents impregnated with anti-clotting agents created more clots). We just donâ€™t know.

The bottom three are different. Number of mistakes, time in surgery, presence of surgical complications. These are things that matter, that surgeons keep track of as metrics of their own performance. And STAR is no better than comparisons here. Much longer theatre time, no significant difference in mistakes or complications.

You are probably a little confused now, because it seems like I just described a different set of plots. STAR looks like it does pretty well in those last three.

STAR is open surgery. A surgeon would immediately understand this, and ignore the LAP and RAS results. It isnâ€™t a fair comparison. They cut a big hole in the pig, and pulled the intestines out through it to repair them. That is a big deal, and comparing it to a human using a laparoscope is like asking them to tie a hand behind their back.Â The risk to the patient is much higher with an open procedure.

We use laparascopic surgery despite slightly higher complications rates because it is better for the patientÂ ifÂ you donâ€™t cut a big, dangerous hole in them.

Compared toÂ human surgeons using an OPEN technique â€¦ STAR underperforms. Three times as long under general anaesthesia is no small thing.

As Andrej Karpathy says â€“ human accuracy is not a point, but a curve. We trade off accuracy against effort. Surgeons donâ€™t bother with millimetre exact stitch spacing, presumably because it doesnâ€™t help. Iâ€™m not up to date on the last hundred years of surgical research, but I am totally happy to take as given that if more careful suturing helped, surgeons would be doing it (or maybe not, often culture trumps evidence).

It is the same thing with predicting cancer survival. Pathologists donâ€™t try toÂ divide people into a dozen survival categories, if all clinical doctors want is to make a binary decision about surgery. It would be overkill. We do what we need to, and no more, and it already costs too much.

So maybe there is a more general rule for deciding when machines beat doctors?

Rule 1b:Â Ask a doctor what they actually do, and what a fair test might be. Doctors trade off accuracy for effort, and optimise for outcomes (be it health, financial, political, cultural etc.)

Does this mean we need to do large randomised control trials to find out if any system actually helps with outcomes?

I wouldnâ€™t go that far. There are certainly tasks I can think of where the causal chain is understood enough to make an accurate inference. For example, in the paper above, luminal reduction post bowel repair has been tested thoroughly enough to know that a 20% or more reduction is needed to have a high chance of symptoms. We can use that as a comparison point. But saying 13% is better than 17% â€¦ we might need further testing to make that claim (or ask a bowel surgeon!).

So that is the first problem I see in â€œsuperhumanâ€ medical systems research. But not all tasks are inappropriately chosen. Some tasks are exactly what doctors do, and we know exactly what doing better would look like. For example,Â Computer Program Beats Doctors at Distinguishing Brain Tumors from Radiation ChangesÂ shows that computers can do better than radiologists at distinguishingÂ radiation necrosis (something that happens after radiotherapy) from brain tumour recurrence. This is very important, very hard for radiologists, and a great target for computational approaches.

Which brings us to the second common error.

AI is AI, right? Machine learning is eating the world? Deep learning is so hot right now? Sure, except when it isnâ€™t.

Not all machine learning is created equal, and not all of it is groundbreaking, even if most people donâ€™t see the difference or think that it matters.

Because the paper in AJNRÂ (again, great paper, important paper) about brain tumours doesnâ€™t use deep learning. This is incredibly common in the radiology literature right now, because some major papers starting in 2010/2011 showed that an old style of image analysis could do some interesting things, like identify tumour subtypes in cancer cases from medical images.

These techniques are not loosely based on the human brain. They donâ€™t â€œseeâ€ the world. They arenâ€™t â€œcognitiveâ€ or â€œintelligentâ€ or whatever other buzzwords are flying around.

These techniques have been around for decades, and we have had the computational power to run them on laptops for almost as long. There has been no hard barrier to doing this work forÂ a long time. So why would it suddenly succeed now, when hundreds or thousands of previous attempts have failed?

Now, that isnâ€™t an argument on its own, but it should beÂ concerning. Non-deep systems donâ€™t exactly have a track record of beating humans at human-like tasks.

The same techniques didnâ€™t beat humans in object recognition. They didnâ€™t help solve Go, or Atari. They didnâ€™t beat human transcriptionists or drive cars safely and autonomously for hundreds of millions of miles. They never left the parking lot.

The old style of image analysis was to get humans to try to describe images with maths, in hand-crafted matrices of numbers. This is super hard, so the best we could do is identify the building blocks of images. Things like edges and small patterns. We could then quantify how much of each pattern was in an image or image region.

This is what they do in the paper.

For starters, you can see why this is so hard for radiologists. A and E look identical.

What they are doing here is taking the region that is brighter (has more fluid in it) and quantifying how much of various textures is present.Â They try over a hundred textures in a cohort of around fifty patients, select the best performing ones and combine them into a predictive signature. They then use that signature to outperform humansÂ to some level of statistical certainty.

Any statistically trained personÂ reading this is hearing alarm bells right now, hopefully.

Probably the biggest problem with using human-defined features is that you will need to test them all, and select the best ones.

Multiple hypothesis testing is a weird beast. I really want to do a blog post on this at some point, because I really do find it strange. But the moral of the story is, if you test lots of hypotheses (â€œtexture x detects cancerâ€ is one such hypothesis), then you get false positives.

Feature selection â€“ choosing the best performing features â€“ probably makes it worse, not better. You expected twenty dodgy results, and you picked the top ten features.

I love the mRMR algorithm used in this paper for feature selection, and use it myself. But dimensionality reduction right at the end isnâ€™t a fix for overfitting. Youâ€™ve already overfit your data. The feature selection helps us explore the predictions and present them, nothing more and nothing less.

The thing is, all researchers understand this. We know that we are probably overfitting when we have very small n and larger p (small sample, more features than samples). We try to mitigate this as best we can, with techniques like hold-out validation sets, cross-validation and so on. This team did all of that. Absolutely perfectly, it is quality work.

But all researchers in this field still know results like this canâ€™t be trusted. Not really. We might not need large randomised clinical trials, but unless a system is tested on a lot moreÂ cases, hopefully from a completely different patient cohort, forget about it.

But donâ€™t take my word for it. Letâ€™s read the paper.

Emphasis mine. The researchers are exactly spot-on here (I honestly canâ€™t think of an example where medical researchers of this calibre have overstated their results).

It isnâ€™t just sample sizes. You can perfectly split your train and test sets, but if you try a dozen different algorithms to see which one works best, you have overfit your data (picture from the Stanford paper again). Which is fine, again, but needs to be recognised.

One more thing to say here, a little more controversial. Public datasets. Be very cautious with public datasets, especially if you have worked with them before or have ever read a paper or blogpost or tweet about someone else working on them. Because you just contaminated your test set, Kagglers. You know what techniques work better than others in this dataset, which has its own idiosyncrasies and biases. The chance of spurious results that fit the bias rather than the true research target is very high.

Many machine learning researchers feel this way about ImageNet, and donâ€™t get very excited by the weekly â€œnew state of the artâ€ results unless there is a big jump in accuracy. Because hundreds of groups areÂ working on that data, trying hundreds of models with wide hyperparameter searches. There is no chance they are not overfitting.

My machine learning colleagues shrug their shoulders. It is just accepted, take each result with a grain of salt and move on. It would just be nice if someone told the journalists and the public.

So, a better formulation for rule 2.

Rule 2a: If it isnâ€™t deep learning, it probably isnâ€™t better than a doctor.

Rule 2b: Overfitting is easy and unavoidable in small and public datasets. Look for larger scale tests, multiple unrelated cohorts, real-world patients.

Whew. Almost there, thanks for sticking around this long.

Type 3 error is easy. The article never even mentions what the headline states, or the article completely misunderstands the research.

Digital Diagnosis: Intelligent Machines Do a Better Job Than HumansÂ from Singularity Hub is a good example. There is not a single mention in the article of a head-to-head comparison. It is all projection and conjecture. It isnâ€™t necessarily a bad article, even. But the headline doesnâ€™t fit.

Artificial Intelligence Reads Mammograms With 99% AccuracyÂ from Futurism is a bit more egregious. This article is about research in using natural language processing. It has nothing to do with reading mammograms, but instead mining the textÂ of the reports that radiologists make. The headline is wrong, and so is a lot of the article.

So where does that leave us?

I remain convinced that we have yet to see a machine outperform a doctor in any task that is relevant to actual medical practice. The slowly building wealth of preliminary research suggests that wonâ€™t last forever, but for now I havenâ€™t seen a case where the robots win.

I hope my rules will be useful, to help distinguish between great research that isnâ€™t quite there yet, and the true breakthroughs that are worth getting very excited about.

And if I have missed a piece of research somewhere, let me know.

Except, while I was writing this â€“ literally this last paragraph â€“Â it became untrue.

Google just published this paperÂ in the Journal of the American Medical Association (impact factor 37 ğŸ™‚ ). And since it actually lives up to the hype, it is a great way to end this piece. Because any worthy set of rules should still work when the situation changes.

They trained a deep learning system to diagnose diabetic retinopathy (damage to the blood vessels in the eye) from a picture of the retina. This is a task that ophthalmologists currently perform using the exact same technique, looking at the retina through a fundoscope.

Googleâ€™s system performed on par with the experts, in a large clinical dataset (130,000 patients). While this isnâ€™t necessarily â€œoutperformingâ€ human doctors, it probably costs under a cent per patient to run the model. An opthalmologist costs a lot more than that, and honestly has better things to do with their time. I am happy to call that a win for the machines.

Letâ€™s look at my rules. Do they work?

Rule 1 â€“ Â is it a task human doctors do, done with the same inputs. Yep.

Rule 2 â€“ is it deep learning, with a decent sized dataset. Yep.

Rule 3 â€“ is it actually a thing? Yep.

So you see, I can be proven wrong with my own system. Science!

As a final note, it is worth looking at why the Google system worked. They paid to make a good dataset. A lot, presumably. They had aÂ panel of between 2 and 7 ophthalmologists grade every single one of the 130,000+ images (from a set of 54 ophthalmologists). That is a huge undertaking. I donâ€™t even know 54 ophthalmologists.

This technology is probably close to ready for a large randomised control trial, and that is a HUGE deal.

This is what we will see in the next few years. There will be many tasks like this, where computers can do exactly what humans do if someone is willing to build the dataset. Most medical tasks probably arenâ€™t right for it, but enough will be thatÂ this will start to happen frequently.|||

If you ask academic machine learning experts about the things that annoy them, high up the list is going to be overblown headlines about how machines are beating humans at some task where that is completely untrue. This is partiallyÂ because reality is already so damn amazing there is no need for hyperbole. AlphaGo beatâ€¦