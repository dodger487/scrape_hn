Researchers from Alibaba and University College London developed a deep learning-based system that learned how to execute a number of strategies for the popular real-time strategy game StarCraft.

Using CUDA, TITAN X and GTX 1080 GPUs and cuDNN with the TensorFlow deep learning framework, the large-scale multiagent system used reinforcement learning to learn strategies employed by high-level players without being given any specific instruction on how best to manage combat. The software improved through trial and error and adapts to changes in the number and type of troops engaged in battle.

“BiCNet (a multiagent bidirectionally-coordinated network) can handle different types of combats under diverse terrains with arbitrary numbers of AI agents for both sides. Our analysis demonstrates that without any supervisions such as human demonstrations or labelled data, BiCNet could learn various types of coordination strategies that is similar to these of experienced game players,” the authors wrote in a paper published to arXiv. “Moreover, BiCNet is easily adaptable to the tasks with heterogeneous agents. In our experiments, we evaluate our approach against multiple baselines under different scenarios; it shows state-of-the-art performance, and possesses potential values for large-scale real-world applications.”

“Real-world artificial intelligence applications often require multiple agents to work in a collaborative effort. Efficient learning for intra-agent communication and coordination is an indispensable step towards general AI,” mentioned the researchers on how this type of system has a broad range of real-world applications.|||

